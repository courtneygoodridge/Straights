---
title: "modelling individual participants"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load data}
# rm(list = ls())

setwd("C:/Users/pscmgo/OneDrive for Business/PhD/Project/Experiment_Code/Straights")
temp = list.files(pattern = c("magnitudedata", "*.csv")) # list all CSV files in the directory
myfiles = lapply(temp, read.csv) # read these CSV in the directory
magnitudedata <- do.call(rbind.data.frame, myfiles) # convert and combine the CSV files into dataframe

```

```{r load packages}

library(ggplot2)
library(dplyr)
library(car)
library(MASS)
library(dplyr)
library(EnvStats)
library(lme4)
library(nlme)
library(tidyr)
library(rstanarm)
library(bayesplot)
library(loo)
library(lmtest)
library(caret)

```

```{r which probability distribution best fits the data}

library(actuar)
library(fitdistrplus)
# inverse <- fitdist(magnitudedata$FirstSteeringTime, "invgauss", start = list(mean = 5, shape = 1))
# qqp(magnitudedata$FirstSteeringTime, "inverse")

# normal distributions
qqp(magnitudedata$FirstSteeringTime, "norm")

# lnorm means lognormal
qqp(magnitudedata$FirstSteeringTime, "lnorm")

# gamma distribution
gamma <- fitdistr(magnitudedata$FirstSteeringTime, "gamma")
qqp(magnitudedata$FirstSteeringTime, "gamma", shape = gamma$estimate[[1]], rate = gamma$estimate[[2]])

# logistic distribution
qqp(magnitudedata$FirstSteeringTime, "logis")

# exponential distribution
qqp(magnitudedata$FirstSteeringTime, "exp")

# extreme distribution
qqp(magnitudedata$FirstSteeringTime, "evd")

# generalised extreme distribution
qqp(magnitudedata$FirstSteeringTime, "gevd")


# nbinom <- fitdistr(recog$Aggression.t, "Negative Binomial")
# qqp(recog$Aggression.t, "nbinom", size = nbinom$estimate[[1]], mu = nbinom$estimate[[2]])
# 
# 
# poisson <- fitdistr(dat2$FirstSteeringTime, "Poisson")
# qqp(dat2$FirstSteeringTime, "pois", poisson$estimate)

```

```{r plotting data to look for linerarity}
library(ggplot2)

ggplot(magnitudedata %>%
         dplyr::filter(heading > 0), aes(x = heading, y = FirstSteeringTime)) +
  geom_jitter(alpha = .1) +
  geom_smooth(method='lm') +
  ggtitle("Normal RT")

ggplot(magnitudedata %>%
         dplyr::filter(heading > 0), aes(x = heading, y = log(FirstSteeringTime))) +
  geom_jitter(alpha = .1) +
  geom_smooth(method='lm') +
  ggtitle("Log transformed RT")

```

```{r median RT per participant}

ggplot(filter(magnitudedata, heading > 0) %>%
         group_by(pNum, heading) %>%
         summarise(medianRT = median(FirstSteeringTime)), aes(x = heading, y = medianRT)) +
  geom_point() +
  geom_line() +
  facet_wrap(~ pNum)

```

Median RTs across heading seem to be fairly consistent across subjects

```{r fitting of multilevel model using lmer function without gaussian link function}

modellingdata <- magnitudedata %>%
  dplyr::filter(heading > 0) %>% # remove 0 heading trials for modelling data
  mutate(inver_heading = 1 / heading) # compute inverse of heading for ease of interpretting intercept

# model 1 - varying intercept
m1 <- lmer(formula = FirstSteeringTime ~ inver_heading + (1 | pNum), data = modellingdata)
summary(m1)
# coef(m1)


# model 2 -  varying intercept and slope
m2 <- lmer(formula = FirstSteeringTime ~ inver_heading + (1 + inver_heading | pNum), data = modellingdata)
summary(m2)
# coef(m2)


# model 3 -  varying intercept and slope
# m3 <- lmer(formula = FirstSteeringTime ~ inver_heading + (1 + heading | pNum), data = modellingdata)
# summary(m2)
# # coef(m2)

par(mfrow = c(2, 2))
plot(fitted(m2),resid(m2, type = "pearson"), col = "blue") # a plot to check the constant standard deviation
abline(h = 0, lwd = 2)

qqnorm(resid(m2)) # normality of the residuals
qqline(resid(m2))

qqnorm(ranef(m2)$pNum[,1]) # normality of the intercepts
qqline(ranef(m2)$pNum[,1])

plot(coef(m2)$pNum)


```

A mixed effects model using lmer function assumes a normal distribution, which is why you cannot select another distribution family for it. 

m1 model formula specifies how FirstSteeringTime is explained by heading. The 1 specifies a varying intercept to vary by participant. coef(m1) shows the different intercepts for each participant. 

m2 model forumula specifies how FirstSteeringTime is explained by heading, except (1 + heading | pNum) allows intercept and slope to vary between participants. Essentially, both the intercept and the slope of heading will vary by pNum, thus accounting for subject variability in baseline fastest RTs and the subject variability in the sensitivity to heading conditions. 

coef(m2) shows the different intercepts for each participant. This mixed model approach allows for estimating of the overall mean response for the explanatory variables whilst adding random deviation based upon the grouping structure i.e. the subjects.

**Residual plot versus fitted values**
This plot possibly shows dependency between fitted and and residual values. A mixed model assumes a normal disstribution thus we should use this plot to investigate whether these assumptions have been violated. Things we should be checking for include:

- Consistent variance above and below the zero line
- No systematic curvature of points
- No indication of a relationship between the axes of the graph

What we see in this plot are more dispered points towards the top of the graph, and a few dispered points towards the bottom, as our fitted values increase. This indicates that the variance in residuals is increasing with fitted values and thus the assumptions of our model could be violated - this is known as hetreoscadastity: when the variance is not equal across different conditions, thus sub-sample variance appears within your data. This could mean that a mixed model is not the best model to use with this given dataset. 

**Residual Q-Q plot**
What are quantiles? - they split data into equal sized groups i.e. the median is the 50% quantile as 50% of the data is above it and 50% is below it. The 75% quantile means that 75% of the data points are below it. 

This plot visualises the normality of the residuals. Sample quantiles relate to the residual points and theoretical quantiles relate to the same number of quantiles generated from the normal distribution. The first quantile for the residuals is plotted against the y axis, and then the first quantile from the normal distribution is plotted on the x axis. Where these lines intersect is where the point is placed.

If the residuals were to be normally distributed, they should all fall on a straight line. Most of the points do, however the tails deviate away from the line indicating slight non-normality of the data. This is line with what our residual versus fitted plot was suggesting.

**Intercept Q-Q plot**
This plot computes the difference between the population-level average predicted response for a given set of fixed-effect values (i.e. heading offset angle) (this relates to the normal distribution theoretical quantiles) and the response predicted for a particular individual (intercept quantiles from the model).

This plot is the same as the residual plot, except we are not plotting intercepts rather than residuals. Essentially, how much does any individual differ from the population? Normal distribution would occur if points were directly on the ine.  

**Coeffiecient plot**
This plots plots coefficients against the intercept, thus visualising the random effects correlation between the intercept and the slope. This visualises the negative correlation (-.39), demonstrating that individuals intercept variation has an effect on the heading slope variation.

```{r solving heteroscadastity using Box-Cox transformation}

# original plot shows evidence of heteroscadasity
plot(m2)

# computing Box-Cox transformation on FirstSteeringTime metric
RT_BCT <- BoxCoxTrans(modellingdata$FirstSteeringTime)

# apply model to metric and then append to old dataframe
modellingdata <- cbind(modellingdata, RT_new = predict(RT_BCT, modellingdata$FirstSteeringTime))

# construct new model with the transformed variable and then create new residual plot
m3 <- lmer(formula = RT_new ~ inver_heading + (1 + inver_heading | pNum), data = modellingdata)
plot(m3)

```

Box-Cox transformation can be applied to your Y variable to make it approximate to a normal distribution.

```{r saving average intercepts and slopes}
# model 1
AvgInterceptm1 <- summary(m1)$coef[1,1]
AvgSlopem1 <- summary(m1)$coef[2,1]

# model 2
AvgInterceptm2 <- summary(m2)$coef[1,1]
AvgSlopem2 <- summary(m2)$coef[2,1]

```


```{r visualising intercepts and slopes for each participant}

interceptsm1 <- coef(m1)$pNum[,1] # Specifying the first column only
slopesm1 <- coef(m1)$pNum[,2] # Specifying the second column only

# varying intercept (slope is fixed)
ggplot(modellingdata, aes(x = inver_heading, y = FirstSteeringTime, color = as.factor(pNum))) + 
  geom_jitter(alpha = .3) +
  geom_abline(slope = slopesm1, intercept = interceptsm1) +
  # geom_abline(slope = AvgSlopem1, intercept = AvgInterceptm1, color = "red", size = 1, show.legend = TRUE) +
  theme(legend.position = "none") +
  theme(axis.text.x = element_blank()) +
  ggtitle("RTs against heading - varying intercept only (model 1)")


interceptsm2 <- coef(m2)$pNum[,1] # Specifying the first column only
slopesm2 <- coef(m2)$pNum[,2] # Specifying the second column only

# varying intercept and slope
ggplot(modellingdata, aes(x = inver_heading, y = FirstSteeringTime, color = as.factor(pNum))) + 
  geom_jitter(alpha = .3) +
  geom_abline(slope = slopesm2, intercept = interceptsm2) +
  # geom_abline(slope = AvgSlopem2, intercept = AvgInterceptm2, color = "red", size = 1, show.legend = TRUE) +
  theme(legend.position = "none") +
  theme(axis.text.x = element_blank()) +
  ggtitle("RTs against heading - varying slope and intercept (model 2)")
  
  
# checking intercepts and slopes for individual subjects (unsure how facet_wrap subjectw with their individual intercepts and slopes...)

# ggplot(magnitudedata %>%
#          dplyr::filter(pNum == 7), aes(x = heading, y = FirstSteeringTime, color = as.factor(pNum))) + 
#   geom_jitter(alpha = .3) +
#   geom_abline(slope = slopesm2[7], intercept = interceptsm2[7], alpha = .5) +
#   geom_abline(slope = AvgSlopem2, intercept = AvgInterceptm2, color = "red", size = 1, show.legend = TRUE) +
#   theme(legend.position = "none") +
#   ggtitle("RTs against heading - varying slope and intercept for an individual")

```

For both plots, heading is inverted. This means 2 degrees of heading offset angle is furtherest to the left. This improves interpretation of the intercept, which should be where subejcts are reacting as quickly as possible.

Varying intercept relates to baseline reaction time i.e. latency for how participants react as fast as they can (when error is instantly above threshold). As expected, this varies a lot between participants. In this case, intercept relates to as fast as possible reactions which will naturally vary amongst the population. 

Allowing the intercept and the slope to vary allows you to see if subjects differ in their sensitivity to heading. First plot proposes that variation in RT is based upon the individuals latency. When we allow the slopes to vary, intercepts become more similar however there is still some variation. There is a slight correlation between intercept and the slope in the random effects output (-.39) which could suggest that variation in the intercept is affecting variation in heading slope (i.e. variation in subjects baseline fastest RTs affects the sensitivity to the heading condition).

What this could mean is that participants who are slower when reacting as fast as possible to an error signal are less sensitive to the experimental manipulation. Alternatively, individuals with noisy RTs may have their intercept pushed up at the regression still tries to account for the mean even with the outliers. A way of teasing this apart would be to investigate whether subjects with higher SDs has flatter slopes and higher intercepts.

```{r comparing SDs to intercept and slope}

RTSD <- modellingdata %>%
  group_by(heading, pNum) %>%
  summarise(RTSD = sd(FirstSteeringTime)) %>%
  ungroup() %>%
  group_by(pNum) %>%
  summarise(RTSD = mean(RTSD))

a <- cbind(RTSD, slopesm2, interceptsm2)

ggplot(a, aes(x = interceptsm2, y = RTSD)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  ggtitle("RTSD plotted against intercept")

ggplot(a, aes(x = slopesm2, y = RTSD)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  ggtitle("RTSD plotted against coefficient")

```

There appears to be little or no correlation between RT standard deviation and intercept. Any correlation that might be there also appears to be negative, rather than positive. Thus it seems that higher intercepts are relating to the a reduced sensitivity to the experimental manipulation.

```{r model comparions}

anova(m1, m2)

```

We can then compare the models by computing a chi squared test. They significantly differ from each other. Model 2 also has lower AIC value, suggesting that it is the best model in explaining the data (i.e. subject variation in sensitivity to the heading).

```{r histogram and density plots}

ggplot() + 
  geom_density(aes(x = slopesm1), col = "blue", show.legend = TRUE) +
  geom_density(aes(x = slopesm2), col = "red", show.legend = TRUE) +
  xlab("slope") +
  ggtitle("Slope density plots")
  

ggplot() + 
  geom_density(aes(x = interceptsm1), col = "blue", show.legend = TRUE) +
  geom_density(aes(x = interceptsm2), col = "red", show.legend = TRUE) +
  xlab("intercept") +
  ggtitle("Intercept density plots")

```

**Slope density pplots**

Blue indicates model 1, red line indicates model 2. Variance in the slopes between the two models is actually very similar according to the density plots.

**Intercept density plots**

Blue indicates model 1, red line indicates model 2. The variance is more spread for model 2 than for model 1. Does this indicate that when you allow the slope to vary according to the heading, variation between the participants increases. Thus subject variability has an affect on the sensitivity towards the heading conditions.

```{r simulating data for testing model fit (taken from Callum's code)}

pp_n <- 20  #number of participants
trials_n <- 160  #number of trials
sigma_y <- 1 # within-participant variability / measurement error

a       <-  0    # average control condition mean
b       <- .5    # average effect size
sigma_a <- 0.06479     # std dev in intercepts (control means, between participant variability)
sigma_b <-  0.07140   # std dev in slopes (effect size)
rho     <-  -0.39   # correlation between intercepts and slopes

# combine means and standard deviations for multivariate gaussian sampling
mu <- c(a, b)
sigmas <- c(sigma_a, sigma_b)          # standard deviations
rho    <- matrix(c(1, rho,             # correlation matrix
                   rho, 1), nrow = 2)

# now matrix multiply to get covariance matrix
sigma <- diag(sigmas) %*% rho %*% diag(sigmas)

#let's create our participant level parameters
set.seed(1)  # used to replicate example
vary_effects <- MASS::mvrnorm(pp_n, mu, sigma) #use multivariate gaussian

#rename columns for varying intercepts and varying slopes
vary_effects <- vary_effects %>% as_tibble() %>% rename(pp_a = V1,
                                                        pp_b = V2)

  
# now simulate observations.

# this piping operation is a little involved, but essential it makes a dataframe of with trial_n entries of all  combinations of participant intercepts and slopes. Then adds a binary column to signify whether the condition is the experimental (1) or control (0). It then uses this column to switch 'on' or 'off' the experimental effect to generate a column of participant condition means. Then it uses the specified (and  constant) within-individual variability (or measurement error) to generate observations

set.seed(1)  # used to replicate example
data <- vary_effects %>% 
  mutate(pp = 1:pp_n) %>% 
  tidyr::expand(nesting(pp, pp_a, pp_b), trial = 1:trials_n) %>% 
  mutate(condition = rep(0:1, times = n() / 2)) %>% 
  mutate(mu = pp_a + pp_b * condition) %>% 
  mutate(measurement = rnorm(n = n(), mean = mu, sd = sigma_y))

head(data)

#quick plot.
ggplot(data, aes(x=measurement, group=condition)) + geom_density()

# 
magnitudedata %>%
  group_by(pNum) %>%
  summarise(n = n()) %>%
  summarise(mean = mean(n))

```

```{r fitting multi-level model for simulated data}

m3 <- lmer(formula = measurement ~ condition + (1 + condition | pp), data = data)
summary(m3)

interceptsm3 <- coef(m3)$pp[,1] # Specifying the first column only
slopesm3 <- coef(m3)$pp[,2] # Specifying the second column only

ggplot(data, aes(x = condition, y = measurement, color = as.factor(pp))) +
  geom_jitter(alpha = .3, show.legend = FALSE) +
  geom_abline(slope = slopesm3, intercept = interceptsm3)

mean(slopesm2)
sd(slopesm2)
sd(interceptsm2)

mean(slopesm3)
sd(slopesm3)
sd(interceptsm3)

```

Model 3 is fitted from the simulated data from Callum's code, based on model 2 that was fitted to actual data. 

**160 trials per participant (average number of trials per participant in real data)**
In comparison to model 2, model 3 has less individual variation. There is also less subject variation towards the sensitivity of the heading condition.

```{r mixed model inference - simulations and 95% CIs}

# plot heading effects 

# Simulate new data based upon the fitted model
new_data <- data.frame(inver_heading = as.numeric(1 / seq(0.5,2,length = 4)))
pred_data <- predict(m2, newdata = new_data, re.form = ~0)

# compute bootstrapped confidence interval, see ?predict.merMod
ci_line <- bootMer(m2, FUN = function(.) predict(.,newdata = new_data, re.form = ~0), nsim = 200)
ci_regT <- apply(ci_line$t, 2, function(x) x[order(x)][c(5,195)])

# plot original data with CIs from simulated data
plot(FirstSteeringTime ~ jitter(inver_heading, 2), modellingdata)
lines(new_data$inver_heading, pred_data, lwd = 3)
lines(new_data$inver_heading, ci_regT[1,], lty = 2)
lines(new_data$inver_heading, ci_regT[2,], lty = 2)
mtext(text = "Regression curves with 95% confidence intervals", side = 3, outer = FALSE, at = -3)

```

This method of inference simulates new reaction time data from the given explanatory variables (inverse heading) and the fitted model. It then re-fits the model to these simulated response values and extracts the fitted coefficient. From the distribution of bootstrapped coefficients, it will draw a 95% confidence intervals (dashed lines) around the fitted values (solid lines).

The small range of the confidence intervals provides increased certainty that the fitted model for these data is valid and reliable. Calculating the confidence intervals from 200 simulations improves this further.

```{r individual regression lines for individual participants}

# create empty vectors for slope coefficients and intercepts
heading_coefs = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)
heading_intercept = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)

for (i in c(1:19)){
  # Create temporary data frame:
heading_tmp <- modellingdata[modellingdata$pNum == i,]
  # Perform regression:
reg_result <- lm(heading_tmp$FirstSteeringTime ~ heading_tmp$inver_heading)
  # Get coefficient:
tmp_coef <- coef(reg_result)

# Store coefficient and intercept for each subject:
heading_coefs[i] <- tmp_coef[2] 
heading_intercept[i] <- tmp_coef[1]

plot(jitter(heading_tmp$inver_heading, 3), heading_tmp$FirstSteeringTime)
abline(heading_intercept[i], heading_coefs[i])

}

# plotting individuals on the same plot
ggplot(modellingdata, aes(x = inver_heading, y = FirstSteeringTime, color = as.factor(pNum))) + 
  geom_jitter(alpha = .3) +
  geom_abline(slope = heading_coefs, intercept = heading_intercept) +
  theme(legend.position = "none") +
  theme(axis.text.x = element_blank()) +
  ggtitle("RTs against heading - individual single level regression slopes")


```

The above code computes a single level regression line for each participant. It them plots the regression line for each subject on their individual RT data points for inverse heading.

The main difference between individual regression models and multi-level models is that  multi-level models assume that coefficients are sampled from a distribution. These assumptions can be useful if there is uncertainty in individual estimates as you can use information about how the individual coefficients vary across the group to make better estimates of indiviual coefficients and thus group averages altogether.

However, if each individual has a lot of data points per condition, we can be more certain of the individual coefficients. Thus we don't have torely on the assumption of them being drawn from a normal distribution, and can extract individual regression lines rather than implement a mixed model.

```{r plotting coefficients from individual regressions vs mixed models}

m2regress <- as.data.frame(cbind(slopesm2, interceptsm2))

m2regress <- m2regress %>%
  mutate(pNum = row_number())

ggplot(m2regress, aes(x = slopesm2)) +
  geom_histogram(aes(y = ..density..)) +
  stat_bin(binwidth = 0.05, bins = 10) +
  stat_function(fun = dnorm, args = with(m2regress, c(mean = mean(slopesm2), sd = sd(slopesm2)))) +
  ggtitle("mixed model coefficients") +
  annotate("text", label = "Skewness = 1.111437, Kurtosis = 1.22957", x = 0.25, y = 10, size = 4)


individualregess <- as.data.frame(cbind(heading_coefs, heading_intercept)) %>%
  mutate(pNum = row_number())

ggplot(individualregess, aes(x = heading_coefs)) +
  geom_histogram(aes(y = ..density..)) +
  stat_bin(binwidth = 0.05, bins = 10) +
  stat_function(fun = dnorm, args = with(individualregess, c(mean = mean(heading_coefs), sd = sd(heading_coefs)))) +
  ggtitle("individual regression line coefficients") +
  annotate("text", label = "Skewness = 1.451552, Kurtosis = 2.601625", x = 0.3, y = 10, size = 4)


skewness(m2regress$slopesm2)
kurtosis(m2regress$slopesm2)

skewness(individualregess$heading_coefs)
kurtosis(individualregess$heading_coefs)

```

The above code looks at the distribution of coefficients of subjects calculated by mixed model and via individual regression lines. The mixed model coefficients are more normally distributed, as would be expected, because they are computed under the assumption that samples are drawn from a normal distribution. 
Conversely, coefficients computed from individual regression lines are more positiviely skewed as they are not under the assumption of distribution normality. 

```{r fitting model to error slowness/decay metric}

# compute metrics
modellingdata <- modellingdata %>%
  mutate(error_decay = 1 / (sin(heading /180 *pi) * 10)) %>%
  mutate(error_growth = sin(heading / 180 * pi) * 10)

# plotting median RTs against error growth
ggplot(modellingdata %>%
         group_by(error_growth) %>%
         summarise(medianRT = median(FirstSteeringTime)), aes(x = error_growth, y = medianRT)) +
  geom_point() +
  geom_line() +
  ggtitle("Median RTs against error growth")

# plotting median RTs against error decay
ggplot(modellingdata %>%
         group_by(error_decay) %>%
         summarise(medianRT = median(FirstSteeringTime)), aes(x = error_decay, y = medianRT)) +
  geom_point() +
  geom_line() +
  ggtitle("Median RTs against error decay")

# plotting heading against error decay

ggplot(modellingdata, aes(x = error_decay, y = heading)) +
  geom_point() +
  geom_line() +
  ggtitle("Heading against error decay")

```

Above code first computes error slowness and error growth.

We already know that that **RT = latency + threshold / sin(heading)**

This means that *RT equals the baseline delay of participants reacting, added to when the error rate exceeds the threshold*. To formulate this into a linear model, the coefficient threshold needs to be multiplied by the error rate. Computing this a rearranging the equation gives us:

*RT = latency + threshold x 1 / sin(heading) x V*

Taking the inverse of sin(heading) generates the metric error decay. Error decay refers to how much the rate of error development has decayed. Conversely, ordinary sin(heading) generates error growth i.e. how much the error rate has developed. Plotting either of these metrics against the median of RT will show whether the relationship is linear or not.

**Plots**
Error growth against median RTs is easier to interpret. Essentially, as error growth increases, median RT decreases. This would be expected, as the faster error develops the faster participants will react to it.

Error decay against median RTs is less intuitive to understand. High error decay value relate to the most decayed error development. At it's highest, there would be no error at all because the development of it would have all decayed away. This will inevitably lead to slow RTs. Conversely, when decay is low there will be larger error development which will lead to faster RTs.

We can confirm this relationship between error decay and RTs by plotting error decay against our raw heading metric. The plot demonstrates that smaller heading have larger error decay. This makes sense, as smaller headings have reduced error development and thus higher "error decay".

