install.packages(compute.es) # for ancova type 3 method
install.packages(multcomp) # post hoc for type 3 method ancova
install.packages(lmPerm) # non-parametric ancova
install.packages(e1071) # for calculating skewness
install.packages(ggpubr) # for interactions plots and boxplots
install.packages(effsize) # for calculating effect sizes
install.packages(gganimate) # animate plots
install.packages(gifski) # animate plots
install.packages(png)
install.packages(transformr)
# install.packages(matlabr)
# install.packages(magrittr)
# full datasets
setwd("M:/PhD/Project/Experiment_Code/Straights/Full_Data") # set working directory on personal laptop
# setwd("C:/Users/Courtney/Documents/PhD/Project/Experiment_code/Straights")
temp = list.files(pattern = c("BenLui17")) # list all CSV files in the directory
myfiles = lapply(temp, read.csv) # read these CSV in the directory
workingdata <- do.call(rbind.data.frame, myfiles) # convert and combine the CSV files into dataframe
# pilot dataset
# setwd("M:/PhD/Project/Experiment_Code/Straights/Pilot")
# file_names <- dir("M:/PhD/Project/Experiment_Code/Straights/Pilot") #where you have your files
# workingdata <- do.call(rbind, lapply(file_names, read.csv))
library(dplyr)
library(tidyr)
workingdata <- unite(workingdata, ppid_trialn, ppid, trialn, sep = "_") # create unique ppid_trialn ID
# sapply(workingdata, class) # identifies class of each column
workingdata$StraightVisible <- as.numeric(workingdata$StraightVisible) # changes factor to numeric, 1 = false, 2 = true
workingdata <- workingdata %>%
group_by(ppid_trialn) %>%
filter(StraightVisible == 2) %>% # filter for only data where central line is visible
mutate(frame = row_number()) # create frame number column
View(workingdata)
library(dplyr)
library(tidyr)
library(TTR)
# library(hemm)
library(searcher)
library(skimr)
# init_hemm()
# change in steering
magnitudethreshold = 0.140
upperthreshold = 0.105 # 0.2184 # upper threshold for consistent steering response
lowerthreshold = 0.005 # lower threshold for when response initiated - lowest of the IQRs for each mean YawRateChange condition
workingdata <- workingdata %>%
mutate(YawRateChange = YawRate_seconds - c(0, YawRate_seconds[-length(YawRate_seconds)])) %>% #difference in yaw rate
group_by(ppid_trialn) %>%
mutate(frame = row_number()) %>%
ungroup()
# workingdata$YawRateChange[workingdata$YawRateChange >= 0.73499999] <- 0 # removes random spikes
# colnames(workingdata)[colnames(workingdata)=="SWA"] <- "SWV"
#
# workingdata <- workingdata %>%
#   mutate(SWA = SWV * 90)
# find out if frames have been dropped
# rows <- workingdata %>%
#   group_by(trialn) %>%
#   summarise(rows = n())
# View(rows)
# workingdata <- workingdata %>%
#   mutate(SWA = SWV * 90)
View(workingdata)
# library(dplyr)
workingdata <- workingdata %>%
group_by(ppid_trialn) %>%
mutate(newYawRateChange = append(diff(YawRateChange),NA))
#quick function for averaging out implausible fluctuations in yaw-rate due to variations in recorded timestamps.
correctYawRateChange<-
function(x,y){ #x needs to be timestamp_diff, y needs to be yaw signal.
Tvec = which(x >.017) # Means dropped frames.
len = length(y)
for (i in 1:length(Tvec)){
p = Tvec[i]
t1 = x[p] #dropped frame.
t2 = x[p+1] #sometimes this is close to zero. If it is, average the two yaw rates. If it isn't, average the preceding and subsequent.
if (p+2 < len){
if (t2 < .015){ ##take the corresponding two datapoints in yaw rate and average
YR1 = y[p+1]
YR2 = y[p+2]
YR_avg = (YR1+YR2) / 2
y[p+1] = YR_avg
y[p+2] = YR_avg
} else { #means it is an isolated dropped frame. average before and after yaw-rates
YR1 = y[p]
YR2 = y[p+2]
YR_avg = (YR1+YR2) / 2
y[p+1] = YR_avg
}
}
}
return(y)
}
workingdata <- workingdata %>%
group_by(ppid_trialn) %>%
mutate(newYawRate = correctYawRateChange(timestamp,YawRateChange))
View(workingdata)
rm(list = ls())
# chooseCRANmirror(graphics=FALSE, ind=1) # uncomment for knitting
# rm(list = ls()) # clear workspace
knitr::opts_chunk$set(echo = TRUE)
install.packages(ggplot2) # for plots
install.packages(dplyr) # for manipulating data frames
install.packages(tidyr) # for tidying data (uniting columns)
install.packages(TTR) # for smoothing data with mean of observations
install.packages(zoo) # package for rolling mean (rollmean function)
install.packages(skimr) # generates summary of data
install.packages(car) # to perform levenes and anova
install.packages(apaTables) # to create apa table
install.packages(WRS2) # for non-parametric anova
install.packages(compute.es) # for ancova type 3 method
install.packages(multcomp) # post hoc for type 3 method ancova
install.packages(lmPerm) # non-parametric ancova
install.packages(e1071) # for calculating skewness
install.packages(ggpubr) # for interactions plots and boxplots
install.packages(effsize) # for calculating effect sizes
install.packages(gganimate) # animate plots
install.packages(gifski) # animate plots
install.packages(png)
install.packages(transformr)
# install.packages(matlabr)
# install.packages(magrittr)
# full datasets
setwd("M:/PhD/Project/Experiment_Code/Straights/Full_Data") # set working directory on personal laptop
# setwd("C:/Users/Courtney/Documents/PhD/Project/Experiment_code/Straights")
temp = list.files(pattern = c("BenLui17")) # list all CSV files in the directory
myfiles = lapply(temp, read.csv) # read these CSV in the directory
workingdata <- do.call(rbind.data.frame, myfiles) # convert and combine the CSV files into dataframe
# pilot dataset
# setwd("M:/PhD/Project/Experiment_Code/Straights/Pilot")
# file_names <- dir("M:/PhD/Project/Experiment_Code/Straights/Pilot") #where you have your files
# workingdata <- do.call(rbind, lapply(file_names, read.csv))
library(dplyr)
library(tidyr)
workingdata <- unite(workingdata, ppid_trialn, ppid, trialn, sep = "_") # create unique ppid_trialn ID
# sapply(workingdata, class) # identifies class of each column
workingdata$StraightVisible <- as.numeric(workingdata$StraightVisible) # changes factor to numeric, 1 = false, 2 = true
workingdata <- workingdata %>%
group_by(ppid_trialn) %>%
filter(StraightVisible == 2) %>% # filter for only data where central line is visible
mutate(frame = row_number()) # create frame number column
library(dplyr)
library(tidyr)
library(TTR)
# library(hemm)
library(searcher)
library(skimr)
# init_hemm()
# change in steering
magnitudethreshold = 0.140
upperthreshold = 0.105 # 0.2184 # upper threshold for consistent steering response
lowerthreshold = 0.005 # lower threshold for when response initiated - lowest of the IQRs for each mean YawRateChange condition
workingdata <- workingdata %>%
mutate(YawRateChange = YawRate_seconds - c(0, YawRate_seconds[-length(YawRate_seconds)])) %>% #difference in yaw rate
group_by(ppid_trialn) %>%
mutate(frame = row_number()) %>%
ungroup()
# workingdata$YawRateChange[workingdata$YawRateChange >= 0.73499999] <- 0 # removes random spikes
# colnames(workingdata)[colnames(workingdata)=="SWA"] <- "SWV"
#
# workingdata <- workingdata %>%
#   mutate(SWA = SWV * 90)
# find out if frames have been dropped
# rows <- workingdata %>%
#   group_by(trialn) %>%
#   summarise(rows = n())
# View(rows)
# workingdata <- workingdata %>%
#   mutate(SWA = SWV * 90)
library(dplyr)
workingdatatimecourseUnsmooth <- workingdata %>%
group_by(ppid_trialn) %>%
mutate(anchored_timestamp = timestamp - min(timestamp)) %>%
filter(anchored_timestamp <= min(anchored_timestamp) + 2.5) %>% # add for seconds fro the original data
ungroup() # create anchored timestamp and then only select first 2.5 seconds of data for each each ppid_trialn (only interested in the first steering adjustment as this point)
# how many frames in each trial?
# rows <- workingdata %>%
#   group_by(ppid, trialn, heading) %>%
#   summarise(rows = n())
#
# View(rows)
#
# rowsgenuine <- rows %>%
#   filter(rows > 270)
# workingdatatimecourseUnsmooth <- workingdatatimecourseUnsmooth %>%
#   group_by(ppid_trialn, heading) %>%
#   filter(frame >= 150)
##################################### SMOOTHING DATA ###################################
# do I need to smooth SWA and calculate a new initial SWA variable for the smoothed data?
workingdatatimecourseSmooth <- workingdatatimecourseUnsmooth %>%
mutate(smoothedYawRateChange = SMA(YawRateChange, n = 3)) # calculates moving average over the number of data points you specify
workingdatatimecourseSmooth[is.na(workingdatatimecourseSmooth)] <- 0 # set NA values to 0. This is because smoothing data in this way creates NA values if you don't have enough to average over. Unsure how else to get around this
# determine first time in each ppid, trialn group above threshold
workingdatathresholdSmooth <- workingdatatimecourseSmooth %>%
group_by(ppid_trialn) %>%
mutate(row_number(), Genuine_Response = if_else(PeakMagnitude >= magnitudethreshold, TRUE, FALSE)) %>%
filter(max(abs(smoothedYawRateChange)) > upperthreshold, # filter largest yaw rate change greater than the upper threshold
min(abs(smoothedYawRateChange)) < lowerthreshold) %>% # filter lowest yaw rate change smaller than the lower theshold
slice(1:min(which(abs(smoothedYawRateChange) > upperthreshold, 1))) %>%
slice(max(which(abs(smoothedYawRateChange) < lowerthreshold, 1))) %>% # which() returns rows where a condition is true. Taking the min() of all the rows that are greater than the criteria will return the first row where that is true.
ungroup() %>%
transmute(ppid_trialn, heading, cameraoffset, SWAThres = SWA, FirstSteeringTime = anchored_timestamp, ThresWorld_x = World_x, ThresWorld_z = World_z, ThresWorldYaw = WorldYaw, ThresYawRate_seconds = YawRate_seconds, gender = gender, age = age, licenseTime = licenseTime, contacts_glasses = contacts_glasses, ThresYawRateChange = smoothedYawRateChange, Genuine_Response = Genuine_Response, PeakMagnitude = PeakMagnitude) %>%
mutate(EarlyResponses = FirstSteeringTime < mean(FirstSteeringTime) - sd(FirstSteeringTime))
rm(list = ls())
# chooseCRANmirror(graphics=FALSE, ind=1) # uncomment for knitting
# rm(list = ls()) # clear workspace
knitr::opts_chunk$set(echo = TRUE)
install.packages(ggplot2) # for plots
install.packages(dplyr) # for manipulating data frames
install.packages(tidyr) # for tidying data (uniting columns)
install.packages(TTR) # for smoothing data with mean of observations
install.packages(zoo) # package for rolling mean (rollmean function)
install.packages(skimr) # generates summary of data
install.packages(car) # to perform levenes and anova
install.packages(apaTables) # to create apa table
install.packages(WRS2) # for non-parametric anova
install.packages(compute.es) # for ancova type 3 method
install.packages(multcomp) # post hoc for type 3 method ancova
install.packages(lmPerm) # non-parametric ancova
install.packages(e1071) # for calculating skewness
install.packages(ggpubr) # for interactions plots and boxplots
install.packages(effsize) # for calculating effect sizes
install.packages(gganimate) # animate plots
install.packages(gifski) # animate plots
install.packages(png)
install.packages(transformr)
# install.packages(matlabr)
# install.packages(magrittr)
# full datasets
setwd("M:/PhD/Project/Experiment_Code/Straights/Full_Data") # set working directory on personal laptop
# setwd("C:/Users/Courtney/Documents/PhD/Project/Experiment_code/Straights")
temp = list.files(pattern = c("BenLui17")) # list all CSV files in the directory
myfiles = lapply(temp, read.csv) # read these CSV in the directory
workingdata <- do.call(rbind.data.frame, myfiles) # convert and combine the CSV files into dataframe
# pilot dataset
# setwd("M:/PhD/Project/Experiment_Code/Straights/Pilot")
# file_names <- dir("M:/PhD/Project/Experiment_Code/Straights/Pilot") #where you have your files
# workingdata <- do.call(rbind, lapply(file_names, read.csv))
library(dplyr)
library(tidyr)
workingdata <- unite(workingdata, ppid_trialn, ppid, trialn, sep = "_") # create unique ppid_trialn ID
# sapply(workingdata, class) # identifies class of each column
workingdata$StraightVisible <- as.numeric(workingdata$StraightVisible) # changes factor to numeric, 1 = false, 2 = true
workingdata <- workingdata %>%
group_by(ppid_trialn) %>%
filter(StraightVisible == 2) %>% # filter for only data where central line is visible
mutate(frame = row_number()) # create frame number column
library(dplyr)
library(tidyr)
library(TTR)
# library(hemm)
library(searcher)
library(skimr)
# init_hemm()
# change in steering
magnitudethreshold = 0.140
upperthreshold = 0.105 # 0.2184 # upper threshold for consistent steering response
lowerthreshold = 0.005 # lower threshold for when response initiated - lowest of the IQRs for each mean YawRateChange condition
workingdata <- workingdata %>%
mutate(YawRateChange = YawRate_seconds - c(0, YawRate_seconds[-length(YawRate_seconds)])) %>% #difference in yaw rate
group_by(ppid_trialn) %>%
mutate(frame = row_number()) %>%
ungroup()
# workingdata$YawRateChange[workingdata$YawRateChange >= 0.73499999] <- 0 # removes random spikes
# colnames(workingdata)[colnames(workingdata)=="SWA"] <- "SWV"
#
# workingdata <- workingdata %>%
#   mutate(SWA = SWV * 90)
# find out if frames have been dropped
# rows <- workingdata %>%
#   group_by(trialn) %>%
#   summarise(rows = n())
# View(rows)
# workingdata <- workingdata %>%
#   mutate(SWA = SWV * 90)
library(dplyr)
workingdatatimecourseUnsmooth <- workingdata %>%
group_by(ppid_trialn) %>%
mutate(anchored_timestamp = timestamp - min(timestamp)) %>%
filter(anchored_timestamp <= min(anchored_timestamp) + 2.5) %>% # add for seconds fro the original data
ungroup() # create anchored timestamp and then only select first 2.5 seconds of data for each each ppid_trialn (only interested in the first steering adjustment as this point)
# how many frames in each trial?
# rows <- workingdata %>%
#   group_by(ppid, trialn, heading) %>%
#   summarise(rows = n())
#
# View(rows)
#
# rowsgenuine <- rows %>%
#   filter(rows > 270)
# workingdatatimecourseUnsmooth <- workingdatatimecourseUnsmooth %>%
#   group_by(ppid_trialn, heading) %>%
#   filter(frame >= 150)
##################################### SMOOTHING DATA ###################################
# do I need to smooth SWA and calculate a new initial SWA variable for the smoothed data?
workingdatatimecourseSmooth <- workingdatatimecourseUnsmooth %>%
mutate(smoothedYawRateChange = SMA(YawRateChange, n = 3)) # calculates moving average over the number of data points you specify
workingdatatimecourseSmooth[is.na(workingdatatimecourseSmooth)] <- 0 # set NA values to 0. This is because smoothing data in this way creates NA values if you don't have enough to average over. Unsure how else to get around this
# Calculates peak magnitude and initial SWA for each trial
workingdatatimecourseSmooth <- workingdatatimecourseSmooth %>%
group_by(ppid_trialn) %>%
mutate(PeakMagnitude = max(abs(YawRateChange))) %>% # calculate peak yaw rate change
ungroup()
# determine first time in each ppid, trialn group above threshold
workingdatathresholdSmooth <- workingdatatimecourseSmooth %>%
group_by(ppid_trialn) %>%
mutate(row_number(), Genuine_Response = if_else(PeakMagnitude >= magnitudethreshold, TRUE, FALSE)) %>%
filter(max(abs(smoothedYawRateChange)) > upperthreshold, # filter largest yaw rate change greater than the upper threshold
min(abs(smoothedYawRateChange)) < lowerthreshold) %>% # filter lowest yaw rate change smaller than the lower theshold
slice(1:min(which(abs(smoothedYawRateChange) > upperthreshold, 1))) %>%
slice(max(which(abs(smoothedYawRateChange) < lowerthreshold, 1))) %>% # which() returns rows where a condition is true. Taking the min() of all the rows that are greater than the criteria will return the first row where that is true.
ungroup() %>%
transmute(ppid_trialn, heading, cameraoffset, SWAThres = SWA, FirstSteeringTime = anchored_timestamp, ThresWorld_x = World_x, ThresWorld_z = World_z, ThresWorldYaw = WorldYaw, ThresYawRate_seconds = YawRate_seconds, gender = gender, age = age, licenseTime = licenseTime, contacts_glasses = contacts_glasses, ThresYawRateChange = smoothedYawRateChange, Genuine_Response = Genuine_Response, PeakMagnitude = PeakMagnitude) %>%
mutate(EarlyResponses = FirstSteeringTime < mean(FirstSteeringTime) - sd(FirstSteeringTime))
# returns TRUE if response is less than 1 SD below mean first steering time
# guards against too fast responses
# mean(workingdatathresholdUnsmooth$FirstSteeringTime) - sd(workingdatathresholdUnsmooth$FirstSteeringTime) calculates the threshold for what is considered too fast i.e. below 0.25 seconds for t4 pilot dataset
# returns TRUE if response is more than 1 SD from zero -> mutate(ResponseTooLate = if_else(FirstSteeringTime > 2*sd(FirstSteeringTime), TRUE, FALSE)) # label late responses
##################################### SMOOTHING DATA ###################################
# Here SWA angle is redundent as it only takes the SWA for when the change in yaw rate is over the threshold... I'm interested in ThresYawRateChange (first change in yaw rate over threhsold) and FirstYawRateChangeTimeThres (first timestamp where yaw rate change is over threshold).
#Only thing extra would be to calculate frame rate (60 frames) (ask Richard), and then multiply by change in yaw rate to get yaw rate per second per second.
library(ggplot2)
library(dplyr)
library(gganimate)
library(gifski)
library(png)
library(transformr)
######## +2.0 heading ##########
ggplot(data = filter(workingdatatimecourseSmooth, heading == -2), aes(x = anchored_timestamp, y = YawRateChange, colour = ppid_trialn)) +
geom_line(show.legend = FALSE) +
geom_vline(data = filter(workingdatathresholdSmooth, heading == -2) %>%
summarise(meanFirstSteeringTime = mean(FirstSteeringTime, na.rm = TRUE)), aes(xintercept = meanFirstSteeringTime)) +
ggtitle("-2 heading - smoothed")
######## +1.5 heading ##########
ggplot(data = filter(workingdatatimecourseSmooth, heading == -1.5), aes(x = anchored_timestamp, y = YawRateChange, colour = ppid_trialn)) +
geom_line(show.legend = FALSE) +
geom_vline(data = filter(workingdatathresholdSmooth, heading == -1.5) %>%
summarise(meanFirstSteeringTime = mean(FirstSteeringTime, na.rm = TRUE)), aes(xintercept = meanFirstSteeringTime)) +
ggtitle("-1.5 heading - smoothed")
######## -1.0 heading ##########
ggplot(data = filter(workingdatatimecourseSmooth, heading == -1), aes(x = anchored_timestamp, y = YawRateChange, colour = ppid_trialn)) +
geom_line(show.legend = FALSE) +
geom_vline(data = filter(workingdatathresholdSmooth, heading == -1) %>%
summarise(meanFirstSteeringTime = mean(FirstSteeringTime, na.rm = TRUE)), aes(xintercept = meanFirstSteeringTime)) +
ggtitle("-1 heading - smoothed")
######## -0.5 heading ##########
ggplot(data = filter(workingdatatimecourseSmooth, heading == -0.5), aes(x = anchored_timestamp, y = YawRateChange, colour = ppid_trialn)) +
geom_line(show.legend = FALSE) +
geom_vline(data = filter(workingdatathresholdSmooth, heading == -0.5) %>%
summarise(meanFirstSteeringTime = mean(FirstSteeringTime, na.rm = TRUE)), aes(xintercept = meanFirstSteeringTime)) +
ggtitle("0.5 heading - smoothed")
######## +0 heading ##########
ggplot(data = filter(workingdatatimecourseSmooth, heading == 0), aes(x = anchored_timestamp, y = YawRateChange, colour = ppid_trialn)) +
geom_line(show.legend = FALSE) +
geom_vline(data = filter(workingdatathresholdSmooth, heading == 0) %>%
summarise(meanFirstSteeringTime = mean(FirstSteeringTime, na.rm = TRUE)), aes(xintercept = meanFirstSteeringTime)) +
ggtitle("0 heading - smoothed")
######## +0.5 heading ##########
ggplot(data = filter(workingdatatimecourseSmooth, heading == 0.5), aes(x = anchored_timestamp, y = YawRateChange, colour = ppid_trialn)) +
geom_line(show.legend = FALSE) +
geom_vline(data = filter(workingdatathresholdSmooth, heading == 0.5) %>%
summarise(meanFirstSteeringTime = mean(FirstSteeringTime, na.rm = TRUE)), aes(xintercept = meanFirstSteeringTime)) +
ggtitle("0.5 heading - smoothed")
######## +1 heading ##########
ggplot(data = filter(workingdatatimecourseSmooth, heading == 1), aes(x = anchored_timestamp, y = YawRateChange, colour = ppid_trialn)) +
geom_line(show.legend = FALSE) +
geom_vline(data = filter(workingdatathresholdSmooth, heading == 1) %>%
summarise(meanFirstSteeringTime = mean(FirstSteeringTime, na.rm = TRUE)), aes(xintercept = meanFirstSteeringTime)) +
ggtitle("+1 heading - smoothed")
######## +1.5 heading ##########
ggplot(data = filter(workingdatatimecourseSmooth, heading == 1.5), aes(x = anchored_timestamp, y = YawRateChange, colour = ppid_trialn)) +
geom_line(show.legend = FALSE) +
geom_vline(data = filter(workingdatathresholdSmooth, heading == 1.5) %>%
summarise(meanFirstSteeringTime = mean(FirstSteeringTime, na.rm = TRUE)), aes(xintercept = meanFirstSteeringTime)) +
ggtitle("1.5 heading - smoothed")
######## +2 heading ##########
ggplot(data = filter(workingdatatimecourseSmooth, heading == 2), aes(x = anchored_timestamp, y = YawRateChange, colour = ppid_trialn)) +
geom_line(show.legend = FALSE) +
geom_vline(data = filter(workingdatathresholdSmooth, heading == 2) %>%
summarise(meanFirstSteeringTime = mean(FirstSteeringTime, na.rm = TRUE)), aes(xintercept = meanFirstSteeringTime)) +
ggtitle("+2 heading - smoothed")
a <- workingdatatimecourseUnsmooth %>%
group_by(ppid_trialn, heading) %>%
filter(YawRateChange < 1)
a %>%
group_by(ppid_trialn, heading) %>%
summarise(rows = n())
a %>%
group_by(ppid_trialn, heading) %>%
filter(frame > 151)
a %>%
group_by(ppid_trialn, heading) %>%
filter(frame < 150)
View(a %>%
group_by(ppid_trialn, heading) %>%
filter(frame < 150))
b <- workingdatatimecourseUnsmooth %>%
filter(ppid_trialn, YawRateChange > 1)
View(workingdatatimecourseUnsmooth)
d <- workingdatatimecourseUnsmooth[c(4,5,27)]
View(d)
d <- workingdatatimecourseUnsmooth[c(4,5,27,28)]
View(d)
b <- filter(workingdatatimecourseUnsmooth$ppid_trialn, YawRateChange > 1)
# chooseCRANmirror(graphics=FALSE, ind=1) # uncomment for knitting
# rm(list = ls()) # clear workspace
knitr::opts_chunk$set(echo = TRUE)
install.packages(ggplot2) # for plots
install.packages(dplyr) # for manipulating data frames
install.packages(tidyr) # for tidying data (uniting columns)
install.packages(TTR) # for smoothing data with mean of observations
install.packages(zoo) # package for rolling mean (rollmean function)
install.packages(skimr) # generates summary of data
install.packages(car) # to perform levenes and anova
install.packages(apaTables) # to create apa table
install.packages(WRS2) # for non-parametric anova
install.packages(compute.es) # for ancova type 3 method
install.packages(multcomp) # post hoc for type 3 method ancova
install.packages(lmPerm) # non-parametric ancova
install.packages(e1071) # for calculating skewness
install.packages(ggpubr) # for interactions plots and boxplots
install.packages(effsize) # for calculating effect sizes
install.packages(gganimate) # animate plots
install.packages(gifski) # animate plots
install.packages(png)
install.packages(transformr)
# install.packages(matlabr)
# install.packages(magrittr)
# full datasets
setwd("M:/PhD/Project/Experiment_Code/Straights/Full_Data") # set working directory on personal laptop
# setwd("C:/Users/Courtney/Documents/PhD/Project/Experiment_code/Straights/Full_Data")
temp = list.files(pattern = c("BenLui17")) # list all CSV files in the directory
myfiles = lapply(temp, read.csv) # read these CSV in the directory
workingdata <- do.call(rbind.data.frame, myfiles) # convert and combine the CSV files into dataframe
# pilot dataset
# setwd("M:/PhD/Project/Experiment_Code/Straights/Pilot")
# file_names <- dir("M:/PhD/Project/Experiment_Code/Straights/Pilot") #where you have your files
# workingdata <- do.call(rbind, lapply(file_names, read.csv))
library(dplyr)
library(tidyr)
workingdata <- unite(workingdata, ppid_trialn, ppid, trialn, sep = "_") # create unique ppid_trialn ID
# sapply(workingdata, class) # identifies class of each column
workingdata$StraightVisible <- as.numeric(workingdata$StraightVisible) # changes factor to numeric, 1 = false, 2 = true
workingdata <- workingdata %>%
group_by(ppid_trialn) %>%
filter(StraightVisible == 2) %>% # filter for only data where central line is visible
mutate(frame = row_number()) # create frame number column
library(dplyr)
library(tidyr)
library(TTR)
# library(hemm)
library(searcher)
library(skimr)
# init_hemm()
# change in steering
magnitudethreshold = 0.140
upperthreshold = 0.080 # 0.105 #  # upper threshold for consistent steering response
lowerthreshold = 0.001 # lower threshold for when response initiated - lowest of the IQRs for each mean YawRateChange condition
workingdata <- workingdata %>%
mutate(YawRateChange = YawRate_seconds - c(0, YawRate_seconds[-length(YawRate_seconds)])) %>% #difference in yaw rate
group_by(ppid_trialn) %>%
mutate(frame = row_number()) %>%
ungroup()
# workingdata$YawRateChange[workingdata$YawRateChange >= 0.73499999] <- 0 # removes random spikes
# colnames(workingdata)[colnames(workingdata)=="SWA"] <- "SWV"
#
# workingdata <- workingdata %>%
#   mutate(SWA = SWV * 90)
# find out if frames have been dropped
# rows <- workingdata %>%
#   group_by(trialn) %>%
#   summarise(rows = n())
# View(rows)
# workingdata <- workingdata %>%
#   mutate(SWA = SWV * 90)
workingdatatimecourseUnsmooth <- workingdata %>%
group_by(ppid_trialn) %>%
mutate(anchored_timestamp = timestamp - min(timestamp)) %>%
filter(anchored_timestamp <= min(anchored_timestamp) + 2.5) %>% # add for seconds fro the original data
ungroup()
library(zoo)
library(dplyr)
library(tidyr)
library(TTR)
# Calculates peak magnitude and initial SWA for each trial
workingdatatimecourseUnsmooth <- workingdatatimecourseUnsmooth %>%
group_by(ppid_trialn) %>%
mutate(PeakMagnitude = max(abs(YawRateChange))) %>% # calculate peak yaw rate change
ungroup()
workingdatathresholdUnsmooth <- workingdatatimecourseUnsmooth %>%
group_by(ppid_trialn) %>%
mutate(row_number(), Genuine_Response = if_else(PeakMagnitude >= magnitudethreshold, TRUE, FALSE), Spike = if_else(PeakMagnitude > 0.8, TRUE, FALSE)) %>%
filter(max(abs(YawRateChange)) > upperthreshold, min(abs(YawRateChange)) < lowerthreshold) %>% # filter lowest yaw rate change smaller than the lower theshold, # filter largest yaw rate change greater than the upper threshold
slice(1:min(which(abs(YawRateChange) > upperthreshold, 1))) %>%
slice(max(which(abs(YawRateChange) < lowerthreshold, 1))) %>% # which() returns rows where a condition is true. Taking the min() of all the rows that are greater than the criteria will return the first row where that is true.
ungroup() %>%
transmute(ppid_trialn, heading, cameraoffset, SWAThres = SWA, FirstSteeringTime = anchored_timestamp, ThresWorld_x = World_x, ThresWorld_z = World_z, ThresWorldYaw = WorldYaw, ThresYawRate_seconds = YawRate_seconds, gender = gender, age = age, licenseTime = licenseTime, contacts_glasses = contacts_glasses, ThresYawRateChange = YawRateChange, Genuine_Response = Genuine_Response, PeakMagnitude = PeakMagnitude, Spike = Spike) %>%
mutate(steering_zscore = scale(FirstSteeringTime)) %>%
mutate(EarlyResponses = steering_zscore <= -1)
# returns TRUE if response is less than 1 SD below mean first steering time
# guards against too fast responses
# mean(workingdatathresholdUnsmooth$FirstSteeringTime) - sd(workingdatathresholdUnsmooth$FirstSteeringTime) calculates the threshold for what is considered too fast i.e. below 0.25 seconds
# returns TRUE if response is more than 1 SD from zero -> mutate(EarlyResponse = if_else(FirstSteeringTime > 2*sd(mean(FirstSteeringTime)), TRUE, FALSE)) # label late responses
ggplot(data = workingdatatimecourseUnsmooth, aes(x = anchored_timestamp, y = YawRateChange, colour = ppid_trialn)) +
geom_line(show.legend = FALSE) +
geom_vline(data = workingdatathresholdUnsmooth %>%
summarise(medianFirstSteeringTime = median(FirstSteeringTime, na.rm = TRUE)), aes(xintercept = medianFirstSteeringTime, color="median"), linetype="dashed") +
facet_wrap( ~ heading)
library(ggplot2)
ggplot(data = workingdatatimecourseUnsmooth, aes(x = anchored_timestamp, y = YawRateChange, colour = ppid_trialn)) +
geom_line(show.legend = FALSE) +
geom_vline(data = workingdatathresholdUnsmooth %>%
summarise(medianFirstSteeringTime = median(FirstSteeringTime, na.rm = TRUE)), aes(xintercept = medianFirstSteeringTime, color="median"), linetype="dashed") +
facet_wrap( ~ heading)
