---
title: "modelling individual participants"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load data}
# rm(list = ls())

# office working directory
setwd("C:/Users/pscmgo/OneDrive for Business/PhD/Project/Experiment_Code/Straights")

# working directory for personal laptop
# setwd("C:/Users/Courtney/Documents/PhD/Project/Experiment_code/Straights")

temp = list.files(pattern = c("magnitudedata", "*.csv")) # list all CSV files in the directory
myfiles = lapply(temp, read.csv) # read these CSV in the directory
magnitudedata <- do.call(rbind.data.frame, myfiles) # convert and combine the CSV files into dataframe

# dput(head(df,n)) provides example of data structure for stack overflow

```


```{r load packages}

library(ggplot2)
library(dplyr)
library(car)
library(MASS)
library(dplyr)
library(EnvStats)
library(lme4)
library(nlme)
library(tidyr)
library(rstanarm)
library(bayesplot)
library(loo)
library(lmtest)
library(caret)
library(merTools)
library(scales)
library(fitdistrplus)
library(mcr) # orthogonal regression
library(quantreg) # quantile regression

```

```{r computing error rate and error growth metrics}

# compute metrics
modellingdata <- magnitudedata %>%
  dplyr::filter(heading > 0) %>% # remove 0 heading trials for modelling data
  mutate(heading_radians = heading / 180 * pi) %>% # convert heading to radians 
  mutate(error_rate = 1 / (sin(heading_radians) * 10)) %>%
  mutate(error_growth = (sin(heading_radians) * 10)) %>%
  mutate(acc_error_rate = 1 / (sqrt(sin(heading)) * 10))

```

```{r comparing new metrics to RT - model explanation}

# plotting median RTs against error growth
ggplot(modellingdata %>%
         group_by(error_growth) %>%
         summarise(medianRT = median(FirstSteeringTime)), aes(x = error_growth, y = medianRT)) +
  geom_point() +
  geom_line() +
  ggtitle("Median RTs against error growth")

# plotting median RTs against error rate
ggplot(modellingdata %>%
         group_by(error_rate) %>%
         summarise(medianRT = median(FirstSteeringTime)), aes(x = error_rate, y = medianRT)) +
  geom_point() +
  geom_line() +
  ggtitle("Median RTs against error rate")

# plotting heading against error rate

ggplot(modellingdata, aes(x = error_rate, y = heading)) +
  geom_point() +
  geom_line() +
  ggtitle("Heading against error rate")

```

Above code first computes error slowness and error growth.

We already know that that **RT = latency + threshold / sin(heading)**

This means that *RT equals the baseline delay of participants reacting, added to when the error growth exceeds the threshold*. To formulate this into a linear model, the coefficient threshold needs to be multiplied by the error. Computing this means rearranging the equation to give us:

*RT = latency + threshold x 1 / sin(heading) x V*

Taking the inverse of sin(heading) generates the metric error rate. Error rate refers to how much time it takes to travel a metre and thus, how slowly or quickly positional error develops. Conversely, ordinary sin(heading) generates error growth i.e. how much the error has developed (how many metres you have travelled in a second). Plotting either of these metrics against the median of RT will show whether the relationship is linear or not.

Computing the inverse therefore makes the regression directly interpretable - the intercept is the latency measured in seconds (i.e. perceptual processing time and motor and mechanical latency) and the slope coefficient represents the threshold measured in metres. In this respect, is it the number of metres travelled away from the line before the threshold is surpassed as subjects react.

We can directly calculate the threhsold for each error rate by by multipling the error rate (speed) by the corresponding Rt (time). This caluclation generates the threshold in metres i.e. how many metres of positional error before subjects are turning the wheel.

**Units**

RT = latency + threshold x error rate, where error rate = 1 / sin(heading_in_radians) x V_in_ms

Error rate = seconds/metre
Error growth = metres/second
RT = seconds
Latency = seconds
Threshold = metres
Threshold * error rate = time to triggering response = seconds

**Plots**
Error growth against median RTs is easier to interpret. Essentially, as error growth increases, median RT decreases. This would be expected, as the faster error develops the faster participants will react to it.

Error rate against median RTs is less intuitive to understand. High error rate values relate slow error development, as more time has passed within the travelled metre. At it's highest, an infinite amount of time would pass before a metre was travelled, because the development would be so slow. This will inevitably lead to slow RTs. Conversely, when error rate is is low, error development will be faster which will lead to faster RTs.

We can confirm this relationship between error rate and RTs by plotting error rate against our raw heading metric. The plot demonstrates that smaller headings have larger error rates. This makes sense, as smaller headings have reduced error development and thus a higher rate of error.


```{r examples of figuring out which distribution best fits my data}

# Poisson distribution example - can only be used on count data i.e. non-negative integer data
fit_p <- fitdist(roaches$y,"pois")
summary(fit_p)
plot(fit_p) 

# Example of what to o if you're not sure on distribution of data. Use summary summary() and plot() to look at individual fits of the data 

# Looks at what distribution that best describes data (does not have every distribution)
# gamma, log normal, Weibull and Beta look the most promising for me
descdist(modellingdata$FirstSteeringTime, discrete = FALSE)

# plot the closest distributions and visually inspect

# gamma distribution
fit_g  <- fitdist(modellingdata$FirstSteeringTime, "gamma")

# log normal distribution - can only be used on positive values (any zeros and it will fail)
fit_ln <- fitdist(modellingdata$FirstSteeringTime, "lnorm")

# weibull distribution
fit_w <- fitdist(modellingdata$FirstSteeringTime, "weibull")

par(mfrow=c(2,2))
plot.legend <- c("Weibull", "gamma", "lognormal")
denscomp(list(fit_w, fit_g, fit_ln), legendtext = plot.legend)
cdfcomp (list(fit_w, fit_g, fit_ln), legendtext = plot.legend)
qqcomp  (list(fit_w, fit_g, fit_ln), legendtext = plot.legend)
ppcomp  (list(fit_w, fit_g, fit_ln), legendtext = plot.legend)



```

Above is an example of how you might work out how your data is distributed. Firstly you need to know what kind of data you have. 

**Poisson distribution** 
Only works for non-negative whole integers has why it is good for data where the you are predicting the number of occurences of a particular event.

**Log normal**
Only for positive values, so no negaive for zero values

**Beta distribution**
Data must be within the [0,1] interval

**Gamma distribution**
Suitable for continuous data that is positive and skewed where the variance is near constant throughout.

Run descdist() function to see roughly where you data falls. The lines closest to the obsered data (blue dot) indicate the distribution which is most like the observed data. Then use fitdistr() function to se distribution fits for individual distribution fits.

For the current modelling data, it appears that gamma, lognormal for weibull distributions will work best.

```{r describing the data}

# histogram distribution for each heading condition
ggplot(magnitudedata, aes(x = FirstSteeringTime)) + geom_histogram(bindwidth = .1) +
  facet_wrap(~heading) + geom_vline(xintercept = .5)

# histogram distribution for 0.5 heading condition for each subject - there is some variability in the distribution of RTs
ggplot(filter(magnitudedata, heading == .5), aes(x = FirstSteeringTime)) + geom_histogram(bindwidth = .1) +
  facet_wrap(~pNum) + geom_vline(xintercept = .5)

# compute overall timestamp, differentiated yaw rate per second variable (YawRateChange) and absolute heading
workingdata2 <- workingdata %>% 
  group_by(ppid_trialn) %>% 
  mutate(timestamp_zero = timestamp - timestamp[1],
         SWA_mirrored = SWA * sign(heading + 1e-6),
         abs_heading = abs(heading),
         diff_YR = c(0, diff(YawRate_seconds)))

# shows the moment that the steering wheel angle is reset before the visible line is presented. For some subjects, there is carry over as they are steering before during and after the reset.

ggplot(dplyr::filter(workingdata2), aes(x = timestamp_zero, y = SWA, group = ppid_trialn, col = ppid_trialn)) +
  geom_line() + 
  guides(colour = FALSE) +
  facet_wrap(~ pNum)

ggplot(dplyr::filter(workingdata2, ppid_trialn == "1_3_88"), aes(x = timestamp_zero, y = WorldYaw)) +
         geom_line() + guides(colour = FALSE)

# plots x and z coordinates for all trials for a specific subject

ggplot(dplyr::filter(workingdata, pNum == 11), aes(x = World_x, y = World_z)) + geom_point()

ggplot(dplyr::filter(workingdata, pNum == 11), aes(x = sign(World_x), y = sign(World_z))) + geom_point()

```


```{r fitting of multilevel model using lmer and glmer functions}

# model 1: linear mixed effect model (varying intercept)
m1 <- lmer(formula = FirstSteeringTime ~ error_rate + (1 | pNum), data = modellingdata)
summary(m1)
# coef(m1)

# plots for model 2
par(mfrow = c(2, 2))
plot(fitted(m1),resid(m1, type = "pearson"), col = "blue") # a plot to check the constant standard deviation
abline(h = 0, lwd = 2)

qqnorm(resid(m1)) # normality of the residuals
qqline(resid(m1))

qqnorm(ranef(m1)$pNum[,1]) # normality of the intercepts
qqline(ranef(m1)$pNum[,1])

plot(coef(m1)$pNum)


# model 2: linear mixed effect model (correlated varying intercept and slope)
m2 <- lmer(formula = FirstSteeringTime ~ error_rate + (1 + error_rate | pNum), data = modellingdata)

# plots for model 2
par(mfrow = c(2, 2))
plot(fitted(m2),resid(m2, type = "pearson"), col = "blue") # a plot to check the constant standard deviation
abline(h = 0, lwd = 2)

qqnorm(resid(m2)) # normality of the residuals
qqline(resid(m2))

qqnorm(ranef(m2)$pNum[,1]) # normality of the intercepts
qqline(ranef(m2)$pNum[,1])

plot(coef(m2)$pNum)

# following function shows canonical link function for gamma distribution. This happens to be inverse link function
args(Gamma) 

# model 3: generalised linear mixed effects model (applying gamma distribution with default inverse link function)
m3 <- glmer(FirstSteeringTime ~ error_rate + (1 + error_rate | pNum), family = Gamma, data = modellingdata)

# plots for model 3
par(mfrow = c(2, 2))
plot(fitted(m3),resid(m3, type = "pearson"), col = "blue") # a plot to check the constant standard deviation
abline(h = 0, lwd = 2)

qqnorm(resid(m3)) # normality of the residuals
qqline(resid(m3))

qqnorm(ranef(m3)$pNum[,1]) # normality of the intercepts
qqline(ranef(m3)$pNum[,1])

plot(coef(m3)$pNum)

# model 4: generalised linear mixed effects model (applying gamma distribution with log link function)
m4 <- glmer(FirstSteeringTime ~ error_rate + (1 + error_rate | pNum), family = Gamma(link = "log"), data = modellingdata)

# plots for model 4
par(mfrow = c(2, 2))
plot(fitted(m4),resid(m4, type = "pearson"), col = "blue") # a plot to check the constant standard deviation
abline(h = 0, lwd = 2)

qqnorm(resid(m4)) # normality of the residuals
qqline(resid(m4))

qqnorm(ranef(m4)$pNum[,1]) # normality of the intercepts
qqline(ranef(m4)$pNum[,1])

plot(coef(m4)$pNum)


# model 5: generalised linear mixed effects model (applying gamma distribution with identity link function)
m5 <- glmer(FirstSteeringTime ~ error_rate + (1 + acc_error_rate | pNum), family = Gamma(link = "identity"), data = modellingdata)

# plots for model 5
par(mfrow = c(2, 2))
plot(fitted(m5),resid(m5, type = "pearson"), col = "blue") # a plot to check the constant standard deviation
abline(h = 0, lwd = 2)

qqnorm(resid(m5)) # normality of the residuals
qqline(resid(m5))

qqnorm(ranef(m5)$pNum[,1]) # normality of the intercepts
qqline(ranef(m5)$pNum[,1])

plot(coef(m5)$pNum)


```

A mixed effects model using lmer function assumes a normal distribution, which is why you cannot select another distribution family for it. This mixed model approach allows for estimating of the overall mean response for the explanatory variables whilst adding random deviation based upon the specified grouping structure i.e. the subjects.

The glmer() function allows you to implement a mixed effects model with different distribution. For example, the currnt data might not be normally distributed and thus a better distribution might fit the data. By using thi function, we can apply different distributions and link functions.

**m1** model formula specifies how FirstSteeringTime is explained by heading. The 1 specifies a varying intercept to vary by participant. This means the model is proposing individual latency values, however the the sensitivity to the manipulated stimulus (error rate) does not vary amongst participants.

**m2** model forumula specifies how FirstSteeringTime is explained by heading. However, (1 + heading | pNum) allows intercept and slope of heading to vary between participants. This accounts for subject variability in baseline RTs (latency) and the subject variability in the sensitivity to heading conditions (threshold). 

**m3** uses the generalised linear model which allows for other distributions to be used to model data. Current data suffers from heteroscadasity and thus cannot assume a normal distribution. **m3** therefore uses a gamma distribution to model data with the default inverse link function.

**m4** again uses the generalised linear model applies a gamma distributions with log link function to the model data.

**m5** again uses the generalised linear model applies a gamma distributions with identity link function to the model data. "Identity" function is essentially like not inputting a link function. 

**Residual plot versus fitted values**
This plot shows dependency between fitted and and residual values. A mixed model assumes a normal disstribution thus we should use this plot to investigate whether these assumptions have been violated. Things we should be checking for include:

- Consistent variance above and below the zero line
- No systematic curvature of points
- No indication of a relationship between the axes of the graph

**m1** and **m2* (models that assume a normal distribution) demonstrate more dispered points towards the top of the graph, and a few dispered points towards the bottom, as our fitted values increase. This indicates that the variance in residuals is increasing with fitted values and thus the assumptions of our model could be violated - this is known as hetreoscadastity: when the variance is not equal across different conditions, thus sub-sample variance appears within your data. This could mean that a mixed model is not the best model to use with this dataset. 

**Residual Q-Q plot**
What are quantiles? - they split data into equal sized groups i.e. the median is the 50% quantile as 50% of the data is above it and 50% is below it. The 75% quantile means that 75% of the data points are below it. 

This plot visualises the normality of the residuals. Sample quantiles relate to the residual points and theoretical quantiles relate to the same number of quantiles generated from the normal distribution. The first quantile for the residuals is plotted against the y axis, and then the first quantile from the normal distribution is plotted on the x axis. Where these lines intersect is where the point is placed.

If the residuals were to be normally distributed, they should all fall on a straight line. For **m1** and **m2**, most of the points do, however the tails deviate away from the line indicating slight non-normality of the data. When we apply gamma distributions to, the normality of the Q-Q plots looks much better.

**Intercept Q-Q plot**
This plot computes the difference between the population-level average predicted response for a given set of fixed-effect values (i.e. heading offset angle) (this relates to the normal distribution theoretical quantiles) and the response predicted for a particular individual (intercept quantiles from the model).

This plot is the same as the residual plot, except we are plotting intercepts rather than residuals. Essentially, how much does any individual differ from the population? Normal distribution would occur if points were directly on the ine.  

**Coeffiecient plot**
This plots plots coefficients against the intercept, thus visualising the random effects correlation between the intercept and the slope. For **m2** this visualises the negative correlation (-.39), demonstrating that individuals intercept variation has an effect on the heading slope variation.

```{r solving heteroscadastity using Box-Cox transformation}

# original plot shows evidence of heteroscadasity
plot(m2)

# if using a linear model, you can test for heteroscadastity - null hypothesis assumes equal variance so a significant p value means there is heteroscadsity present.
linearmodelexample <- lm(FirstSteeringTime ~ inver_heading, data = modellingdata)

bptest(linearmodelexample)  # Breusch-Pagan test
ncvTest(linearmodelexample) # Non-constant Variance Score Test 

# computing Box-Cox transformation on FirstSteeringTime metric
RT_BCT <- BoxCoxTrans(modellingdata$FirstSteeringTime)

# apply model to metric and then append to old dataframe
modellingdata <- cbind(modellingdata, RT_new = predict(RT_BCT, modellingdata$FirstSteeringTime))

# construct new model with the transformed variable and then create new residual plot
mBoxCox <- lmer(formula = RT_new ~ inver_heading + (1 + inver_heading | pNum), data = modellingdata)
plot(m3)

```

Box-Cox transformation can be applied to your Y variable to make it approximate to a normal distribution. This will be necessary for future computations, such as prediction intervals.

```{r visualising intercepts and slopes for each participant alongside averages}

# individual and average slopes and intercepts
AvgInterceptm1 <- summary(m1)$coef[1,1]
AvgSlopem1 <- summary(m1)$coef[2,1]

interceptsm1 <- coef(m1)$pNum[,1] # Specifying the first column only
slopesm1 <- coef(m1)$pNum[,2] # Specifying the second column only

# varying intercept (slope is fixed) - plotting for all subjects
ggplot(modellingdata, aes(x = error_rate, y = FirstSteeringTime, color = as.factor(pNum))) + 
  geom_jitter(alpha = .3) +
  geom_abline(slope = slopesm1, intercept = interceptsm1) +
  geom_abline(slope = AvgSlopem1, intercept = AvgInterceptm1, color = "red", size = 1, show.legend = TRUE) +
  theme(legend.position = "none") +
  theme(axis.text.x = element_blank()) +
  ggtitle("RTs against heading - varying intercept only (model 1)")


# individual and average slopes and intercepts
AvgInterceptm2 <- summary(m2)$coef[1,1]
AvgSlopem2 <- summary(m2)$coef[2,1]

interceptsm2 <- coef(m2)$pNum[,1] # Specifying the first column only
slopesm2 <- coef(m2)$pNum[,2] # Specifying the second column only

# varying intercept and slope  - plotting for all subjects
ggplot(modellingdata, aes(x = error_rate, y = FirstSteeringTime, color = as.factor(pNum))) + 
  geom_jitter(alpha = .3) +
  geom_abline(slope = slopesm2, intercept = interceptsm2) +
  geom_abline(slope = AvgSlopem2, intercept = AvgInterceptm2, color = "red", size = 1, show.legend = TRUE) +
  theme(legend.position = "none") +
  theme(axis.text.x = element_blank()) +
  ggtitle("RTs against heading - varying slope and intercept (model 2)")
  
```

For both plots, error rate is plotted against FirstSteeringTime. This way, intercept and slope are directly interpretable as latency and threshold. 

Varying intercept relates to baseline reaction time i.e. latency for how participants react as fast as they can (when error is instantly above threshold). As expected, this varies a lot between participants. In this case, intercept relates to as fast as possible reactions which will naturally vary amongst the population. 

Allowing the intercept and the slope to vary allows you to see if subjects differ in their sensitivity to heading. First plot proposes that variation in RT is based upon the individuals latency. When we allow the slopes to vary, intercepts become more similar however there is still some variation. There is a slight correlation between intercept and the slope in the random effects output (-.39) which could suggest that variation in the intercept is affecting variation in heading slope (i.e. variation in subjects baseline fastest RTs affects the sensitivity to the heading condition).

What this could mean is that participants who are slower when reacting as fast as possible to an error signal are less sensitive to the experimental manipulation. Alternatively, individuals with noisy RTs may have their intercept pushed up at the regression still tries to account for the mean even with the outliers. A way of teasing this apart would be to investigate whether subjects with higher SDs has flatter slopes and higher intercepts.

```{r comparing SDs to intercept and slope}

RTSD <- modellingdata %>%
  group_by(heading, pNum) %>%
  summarise(RTSD = sd(FirstSteeringTime)) %>%
  ungroup() %>%
  group_by(pNum) %>%
  summarise(RTSD = mean(RTSD))

a <- cbind(RTSD, slopesm2, interceptsm2)

ggplot(a, aes(x = interceptsm2, y = RTSD)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  ggtitle("RTSD plotted against intercept")

ggplot(a, aes(x = slopesm2, y = RTSD)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  ggtitle("RTSD plotted against coefficient")

```

There appears to be little or no correlation between RT standard deviation and intercept. Any correlation that might be there also appears to be negative, rather than positive. Thus it seems that higher intercepts are relating to reduced sensitivity to the experimental manipulation.

```{r model comparions}

anova(m1, m2, m3, m4, m5)

```

We can then compare the models by using an ANOVA to computer a chi squared statistic. All the model significantly explain the data. *m4* has the lowest AIC value, suggesting that it is the best model in explaining the data. This model had a varying intercept and slope with gamma distribution with log link function. 

```{r histogram and density plots}

ggplot() + 
  geom_density(aes(x = slopesm1), col = "blue", show.legend = TRUE) +
  geom_density(aes(x = slopesm2), col = "red", show.legend = TRUE) +
  xlab("slope") +
  ggtitle("Slope density plots")
  

ggplot() + 
  geom_density(aes(x = interceptsm1), col = "blue", show.legend = TRUE) +
  geom_density(aes(x = interceptsm2), col = "red", show.legend = TRUE) +
  xlab("intercept") +
  ggtitle("Intercept density plots")

```

**Slope density pplots**

Blue indicates model 1, red line indicates model 2. Variance in the slopes between the two models is actually very similar according to the density plots.

**Intercept density plots**

Blue indicates model 1, red line indicates model 2. The variance is more spread for model 2 than for model 1. Does this indicate that when you allow the slope to vary according to the heading, variation between the participants increases. Thus subject variability has an affect on the sensitivity towards the heading conditions.


```{r creating prediction and confidence intervals for individual regression lines}

# create empty vectors for slope coefficients and intercepts
heading_coefs = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)
heading_intercept = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)

for (i in c(1:19)){
  # Create temporary data frame:
heading_tmp <- modellingdata[modellingdata$pNum == i,]
  # Perform regression:
reg_result <- lm(FirstSteeringTime ~ error_rate, data = heading_tmp)
  # Get coefficient:
tmp_coef <- coef(reg_result)
  # Get model prediction
tmp_pred <- predict(reg_result, interval = "prediction")

# Store coefficient and intercept for each subject:
heading_coefs[i] <- tmp_coef[2] 
heading_intercept[i] <- tmp_coef[1]

# Store prediction intervals
heading_tmp <- cbind(heading_tmp, tmp_pred)

# base R plotting
# plot(jitter(heading_tmp$error_rate, 3), heading_tmp$FirstSteeringTime)
# abline(heading_intercept[i], heading_coefs[i])
# lines(x = heading_tmp$error_rate, y = heading_tmp$upr, type = "l", lty = 1, col = "red")
# lines(x = heading_tmp$error_rate, y = heading_tmp$lwr, type = "l", lty = 1, col = "red")

# ggplot plotting
print(ggplot(heading_tmp, aes(x = acc_error_rate, y = FirstSteeringTime)) +
  geom_jitter() +
  stat_smooth(method = lm) +
  geom_line(aes(y = lwr), color = "red", linetype = "dashed") +
  geom_line(aes(y = upr), color = "red", linetype = "dashed") +
  geom_line(aes(y = fit), color = "black", linetype = "dashed") +
  ylim(0, 2.5))

}

# plot(heading_intercept, heading_coefs)
# abline(lm(heading_coefs ~ heading_intercept), col = "red")

# plotting individuals on the same plot

# ggplot(modellingdata, aes(x = error_rate, y = FirstSteeringTime, color = as.factor(pNum))) + 
#   geom_jitter(alpha = .3) +
#   geom_abline(slope = heading_coefs, intercept = heading_intercept) +
#   theme(legend.position = "none") +
#   theme(axis.text.x = element_blank()) +
#   ggtitle("RTs against heading - individual single level regression slopes")

```

The above code computes a single level regression line for each participant. It them plots the regression line for each subject on their individual RT data points for inverse heading.

Once each linear model has been computed, it uses this model to create prediction intervals for each subject. The prediction intervals give an estimate of where an RT might line for a subject for any given error rate i.e. an error rate that was not sampled in the experiment. 

The main difference between individual regression models and multi-level models is that  multi-level models assume that coefficients are sampled from a distribution. These assumptions can be useful if there is uncertainty in individual estimates. This is because you can use information from the distribution (how the individual coefficients vary across a sample) to make better estimates of indiviual coefficients and thus group averages altogether.

However, if each individual has a lot of data points per condition, we can be more certain of the individual coefficients. The current data set has a lot of data points per condition, thus we don't have to rely on the assumption of them being drawn from a normal distribution. We can therefore extract individual regression lines rather than implement a mixed model.

**Notes on scatterplot and regression line and potential for orthogonal regression**

Regression line is directly interpretable as the threshold for each subject, which is the same across each heading condition for subjects. This can be computed via:

Threshold = (RT - latency = triggered time) / error rate. 

However, some subjects does not have a large slope and thus have extremely small threshold values

One reason for this could be to do with performing a normal regression. A normal regression line aims to reduce residuals by reducing the vertical distance between the y value (RT) and the fitted y values (regression line). This is usally fine, however in my experiment and analysis there is a difference in my x axis variable. 

For my analysis, I am considering heading as a factorial variable whereas in reality (subject observation) is was a continous variable (experimental manipulation + carry over). Hence becuase I am treating my regressors as factorial, this carry over error gets pushed onto the y intercept and thus can be reduce the slope. Alternatively, an orthogonal regression generates a regression line that minimises distance across x and y axes for the fitted line. This means the error can be accounted for a more valid slope (threshold) for each subject can be generated.  

Orthogonal/total regression package - passing-bablok

**Computing parameters within the threshold model**

We know that: 

RT = latency + threshold * error rate, so therefore

RT - latency = threshold * error rate

Threshold = (RT - latency) / error rate

Error rate = (RT - latency) / threshold

Latency = RT - threshold * error rate.

Subtracting the intercept value (latency) from the RT generates the triggered response time i.e. how long before subjects realised they needed to make a response. Some of these values are negative, suggested that the model is predicting latency values that are longer than what people are actually responding... on a qualitative level, this could indicate the model does not describe the data well.

**Sidenote about plotting** when using base R plot function, I need to calculate and save the individual slopes and intercepts. However when using ggplot, I can using the stat_smooth function and compute a linear model for each subject that way.

**Difference between CIs and prediction intervals**

Confidence intervals tell you how well you have calculated the population mean value. Imagine you sample from a Gaussian distribution a number of times and generate confidence intervals for each sample mean. You would expect 95% of those intervals to contain the population mean. Confidence intervals tell you about the likely location of the population parameter. Thus the linear regression line is likely to to fall within the boundary of the CIs 95% of the time.

Prediction intervals tell you where you would expect to see the next data point that has been sampled for a new observation for that specific subject. For example, if a subject were subjected to an error rate different to the ones used in the experiment, what would be their reaction time. Because they have to deal with a single point rather than the mean of values, predicition intervals are usually wider than confidence intervals. One thing to note is that predicction intervals rely strongly on the assumption of constant variance.

**Confidence interval output**

**Fit:** the predicted population level mean values for the three error rate values
**lwr and upr:** the lower and the upper confidence limits for the expected values, respectively. By default the function produces the 95% confidence limits.

**Predicition interval output**

**Fit** refers to the centre of the distribution i.e. a predicition of each value based upon the model
**Upper** and **lower** refer to the upper and lower bounds of the predicition intervals, whereby 95% of subjects would fall within these predicition interval bounds for a given error rate value

```{r inputting individual regression/multi level model intercepts into modelling dataframe}

# obtain coefficients and then intercepts from multi level model 
m5_coefficients <- coef(m5)[["pNum"]]
heading_intercept_m5 <- m5_coefficients[,1]
heading_slope_m5 <- m5_coefficients[,2]

# for loop and if statement to input them into modellingdata dataframe
for (i in c(1:19)){
  if (modellingdata$pNum == i){
    modellingdata$individual_intercept <- heading_intercept[modellingdata$pNum]
    modellingdata$multilevel_intercept <- heading_intercept_m5[modellingdata$pNum]
    modellingdata$individual_threshold <- heading_coefs[modellingdata$pNum]
    modellingdata$multilevel_threshold <- heading_slope_m5[modellingdata$pNum]
  }
}

```


```{r prediction intervals for mixed models - one participant to make sure it works}

# fit glm for one participant
pp_RT <- filter(modellingdata, pNum == 11)

pp_RT_m1 <- glm(data = pp_RT, FirstSteeringTime ~ error_rate, family = Gamma(link = identity))

# compute 100 new simulated error rates
pd <- with(modellingdata,
           data.frame(error_rate = seq(min(error_rate), max(error_rate),
                                       length = 100)))

# compute prediction intervals for new error rates based upon glm model
pd <- cbind(pd, predict(pp_RT_m1, pd, type = "response", se.fit = TRUE)[1:2])

# compute upper bound by adding the fit to 1.96 (95% interval) multiplied by the standard error of the fit (standard deviation estimate). Compute lower bound by subtracting from the same calculation.
pd <- pd %>% 
  mutate(upper = fit + (1.96 * se.fit),
         lower = fit - (1.96 * se.fit))

ggplot(pp_RT, aes(x = error_rate, y = FirstSteeringTime)) +
  geom_line(data = pd, aes(y = lower, x = error_rate), color = "red", linetype = "dashed") + 
  geom_line(data = pd, aes(y = upper, x = error_rate), color = "red", linetype = "dashed") +
  geom_line(data = pd, aes(y = fit, error_rate), color = "black", linetype = "dashed") +
  geom_jitter(alpha = .2)


```


```{r predictions for GLM for multiple participants}

# create empty vectors for slope coefficients and intercepts
heading_coefs = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)
heading_intercept = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)

for (i in c(1:19)){
  # Create temporary data frame:
heading_tmp <- modellingdata[modellingdata$pNum == i,]
  # Perform regression:
reg_result <- glm(FirstSteeringTime ~ error_rate, family = Gamma(link = identity), data = heading_tmp)
  # Get coefficient:
tmp_coef <- coef(reg_result)

  # generate 100 simulated error rates between minimum and maximum error rate that were sampled in the experiment
tmp_pred <- with(modellingdata,
           data.frame(error_rate = seq(min(error_rate), max(error_rate),
                                       length = 100)))

# computed predictions of fit and standard error for simulated error rates
tmp_pred<- cbind(tmp_pred, predict(reg_result, tmp_pred, type = "response", se.fit = TRUE)[1:2])

# calculate upper and lower bounds 
tmp_pred <- tmp_pred %>% 
  mutate(upper = fit + (1.96 * se.fit),
         lower = fit - (1.96 * se.fit))

# Store coefficient and intercept for each subject:
heading_coefs[i] <- tmp_coef[2] 
heading_intercept[i] <- tmp_coef[1]

# base R plotting
# plot(jitter(heading_tmp$error_rate, 3), heading_tmp$FirstSteeringTime)
# abline(heading_intercept[i], heading_coefs[i])#
# lines(x = tmp_pred$error_rate, y = tmp_pred$fit, type = "l", lty = 1, col = "black")
# lines(x = tmp_pred$error_rate, y = tmp_pred$upper, type = "l", lty = 1, col = "red")
# lines(x = tmp_pred$error_rate, y = tmp_pred$lower, type = "l", lty = 1, col = "red")


# ggplot plotting
print(ggplot(heading_tmp, aes(x = error_rate, y = FirstSteeringTime)) +
  geom_jitter() +
  stat_smooth(method = "glm") +
  geom_line(data = tmp_pred, aes(x = error_rate, y = lower), color = "red", linetype = "dashed") +
  geom_line(data = tmp_pred, aes(x = error_rate, y = upper), color = "red", linetype = "dashed") +
  geom_line(data = tmp_pred, aes(x = error_rate, y = fit), color = "black", linetype = "dashed"))

}

```

Above code computes prediction intervals and fits for the glm model that uses a gamma distribution with a log link function.

Firstly, the a model is computed for each subject using the glm function. A gamma distribution with log link function is specified. 

Predicition intervals were then calculated. Firstly, 100 error rates were simulated between the minimum and maximum error rates that were sampled in the experiment. Average fits and standard errors for each simulated error rate were then computed. Following this, the upper and lower bounds of the predicition intervals were caluclated. This is done by adding (upper) or subtracting (lower) 1.96 * standard error of fit from the original fit value. Multipling 1.96 by the standard error of fit calculates the 95% distribution of values for error rate. This is because 1.96 standard deviations either side of the mean represents 95% of the population, and the standard error is a measure of the deviation of fit across the sampling distribution.

Once these have been computed, they are plotted alongside the confidence intervals and the glm.**Sidenote** as with the individual lm's, when using base R plot function, I need to calculate and save the individual slopes and intercepts. However when using ggplot, I can using the stat_smooth function and compute a generalised linear model for each subject that way.

**Comparison of prediction intervals to the linear models**

Prediction intervals are much narrower and encompass less of the data. Because they encompass less of the data, can this model be consider a "worse model"? 

They are also not parallel with each other which may represent the fact that they take into account the varying variance? 




```{r plotting coefficients from individual regressions vs mixed models - how do distributions differ?}

m2mixedmodel <- as.data.frame(cbind(slopesm2, interceptsm2))

m2mixedmodel <- m2mixedmodel %>%
  mutate(pNum = row_number())

ggplot(m2mixedmodel, aes(x = slopesm2)) +
  geom_histogram(aes(y = ..density..)) +
  stat_bin(binwidth = 0.05, bins = 10) +
  stat_function(fun = dnorm, args = with(m2mixedmodel, c(mean = mean(slopesm2), sd = sd(slopesm2)))) +
  ggtitle("mixed model coefficients") +
  annotate("text", label = "Skewness = 1.111437, Kurtosis = 1.22957", x = 0.25, y = 10, size = 4)


individualmodel <- as.data.frame(cbind(heading_coefs, heading_intercept)) %>%
  mutate(pNum = row_number())

ggplot(individualmodel, aes(x = heading_coefs)) +
  geom_histogram(aes(y = ..density..)) +
  stat_bin(binwidth = 0.05, bins = 10) +
  stat_function(fun = dnorm, args = with(individualmodel, c(mean = mean(heading_coefs), sd = sd(heading_coefs)))) +
  ggtitle("individual regression line coefficients") +
  annotate("text", label = "Skewness = 1.451552, Kurtosis = 2.601625", x = 0.3, y = 10, size = 4)


skewness(m2mixedmodel$slopesm2)
kurtosis(m2mixedmodel$slopesm2)

skewness(individualmodel$heading_coefs)
kurtosis(individualmodel$heading_coefs)

```

The above code looks at the distribution of coefficients of subjects calculated by mixed model and via individual regression lines. The mixed model coefficients are more normally distributed, as would be expected, because they are computed under the assumption that samples are drawn from a normal distribution.

Conversely, coefficients computed from individual regression lines are more positiviely skewed as they are not under the assumption of distribution normality. 

```{r mixed model inference - simulations and 95% CIs on error rate metric}

m2 <- lmer(formula = FirstSteeringTime ~ error_rate + (1 + error_rate | pNum), data = modellingdata)

# plot heading effects 

# Simulate new data based upon the fitted model
new_data <- data.frame(heading = as.numeric(seq(0.5, 2, length = 4)))
new_data <- new_data %>%
  mutate(heading_radians = heading / 180 * pi) %>%
  mutate(error_rate =  1 / (sin(heading_radians) * 10))

pred_data <- predict(m2, newdata = new_data, re.form = ~0)

# compute bootstrapped confidence interval, see ?predict.merMod
ci_line <- bootMer(m2, FUN = function(.) predict(.,newdata = new_data, re.form = ~0), nsim = 200)
ci_regT <- apply(ci_line$t, 2, function(x) x[order(x)][c(5,195)])

# plot original data with CIs from simulated data
plot(FirstSteeringTime ~ jitter(error_rate, 2), modellingdata)
lines(new_data$error_rate, pred_data, lwd = 3)
lines(new_data$error_rate, ci_regT[1,], lty = 2)
lines(new_data$error_rate, ci_regT[2,], lty = 2)

```

Simulates new reaction time data from the given explanatory variables (error rate) and the fitted model. It then re-fits the model to these simulated response values and extracts the fitted coefficient. From the distribution of bootstrapped coefficients, it will draw a 95% confidence intervals (dashed lines) around the fitted values (solid lines).

The small range of the confidence intervals provides increased certainty that the fitted model for these data is valid and reliable. Calculating the confidence intervals from 200 simulations improves this further.

```{r mixed model inference - predicition interval for error rate metric}

fit <- lmer(formula = FirstSteeringTime ~ error_rate + (1 + error_rate | pNum), data = modellingdata)
pred <- cbind(modellingdata, predictInterval(fit, modellingdata))

ggplot(pred, aes(x = error_rate, y = fit)) + 
  stat_summary(fun.y = mean, geom = "line", show.legend = FALSE) + 
  # stat_summary(fun.ymin = mean, geom = "ribbon", show.legend = FALSE) +
  # stat_summary(fun.ymax = mean, geom = "ribbon", show.legend = FALSE) +
  geom_ribbon(aes(x = error_rate, ymin = lwr, ymax = upr), alpha = 0.2) +
  geom_jitter(aes(x = error_rate, y = FirstSteeringTime), alpha = 0.1) +
  ylab("RT")

```

predictInterval function computes new simulated distribution of parameters from the model. The distribution is then cut using median or mean stat functions and then is returned alongside the upper and lower bounds of the prediction.

**Fit** refers to the centre of the distribution i.e. a predicition of each value based upon the model

**Upper** and **lower** refer to the upper and lower bounds of the predicition intervals, whereby 95% of subjects would fall witHin these predicition interval bounds for a given error rate value

Stat summary function needed to average fit and lower and upper bounds in order to avoid vertical lines for each real error rate value within the dataset.


```{r predicting RTs for new heading values using a linear model}

model2 <- lm(FirstSteeringTime ~ error_rate, data = modellingdata)

new.data <- data.frame(error_rate = c(15.0, 20.0, 25.0)) # headings
new.data <- new.data %>%
  mutate(error_rate = error_rate / 180 * pi) %>% # convert to radians
  mutate(error_rate = 1 / (sin(error_rate) * 10)) # convert to error rate

# predict function predicts RTs for new data error rates
predict(model2, newdata = new.data, interval = "prediction")

```

