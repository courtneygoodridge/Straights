---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load packages}
library(ggplot2)
library(dplyr)
library(car)
library(MASS)
library(dplyr)
library(EnvStats)
library(lme4)
library(nlme)
library(tidyr)
library(rstanarm)
library(bayesplot)
library(loo)
library(lmtest)
library(caret)
library(merTools)
library(scales)
library(fitdistrplus)
library(mcr) # orthogonal regression
library(quantreg) # quantile regression
```


```{r orthogonal regression}

example <- modellingdata %>%
  filter(pNum == 12)

# linear regression for least squares method

lin.reg <- lm(FirstSteeringTime ~ error_rate, data = example)
plot(jitter(example$error_rate, 3), example$FirstSteeringTime, col = adjustcolor("black", alpha = .3), main = "Linear regression") 
abline(lin.reg, col = "red")

# Deming regression example

dem.reg <- mcreg(example$error_rate, example$FirstSteeringTime, method.reg = "Deming")
str(dem.reg)
dem.reg@para
plot(jitter(example$error_rate, 3), example$FirstSteeringTime, col = adjustcolor("black", alpha = .3), main = "Deming regression")
abline(dem.reg@para[1:2], col = "red")

# Weighted Deming regression example

w.dem.reg <- mcreg(example$error_rate, example$FirstSteeringTime, method.reg = "WDeming")
w.dem.reg@para

plot(jitter(example$error_rate, 3), example$FirstSteeringTime, col = adjustcolor("black", alpha = .2), main = "Deming and weighted deming regression") 
abline(dem.reg@para[1:2], col = "blue")
abline(w.dem.reg@para[1:2], col = "green")
legend("topleft", c("Deming","Weighted Deming"), lty=c(1,1), col = c("blue","green"))

# Passing Bablock regression example

PB.reg <- mcreg(example$error_rate, example$FirstSteeringTime, method.reg = "PaBa")
PB.reg@para
plot(jitter(example$error_rate, 3), example$FirstSteeringTime, col = adjustcolor("black", alpha = .3), main = "Deming regression")
abline(PB.reg@para[1:2], col = "red")

# Quantile regression
QR.reg <- rq(example$FirstSteeringTime ~ example$error_rate)
plot(jitter(example$error_rate, 3), example$FirstSteeringTime, col = adjustcolor("black", alpha = .3), main = "Quantile regression") 
abline(QR.reg, col = "red")

# Iteratively re-weighted least squares fit
it.reg <- rlm(example$FirstSteeringTime ~ example$error_rate)
plot(jitter(example$error_rate, 3), example$FirstSteeringTime, col = adjustcolor("black", alpha = .3), main = "Re-weighted regression") 
abline(it.reg, col = "red")

# comparing all the regressions
plot(jitter(example$error_rate, 3), example$FirstSteeringTime, col = adjustcolor("black", alpha = .3), main = "Regression comparison")
abline(lin.reg, col = "red")
abline(dem.reg@para[1:2], col = "blue")
abline(w.dem.reg@para[1:2], col = "green")
abline(PB.reg@para[1:2], col = "black")
legend("topleft", c("Linear", "Deming", "Weighted Deming", "Passing-Bablok"), lty=c(1,1), col = c("red", "blue","green", "black"))

# comparing quantile, passing-bablok and re-weighted least squares regression
plot(jitter(example$error_rate, 3), example$FirstSteeringTime, col = adjustcolor("black", alpha = .3), main = "Quantile versus Passing-Bablok") 
abline(lin.reg, col = "red")
abline(PB.reg@para[1:2], col = "black")
abline(it.reg, col = "green")
legend("topleft", c("Quantile", "Passing-Bablok", "Re-weighted"), lty=c(1,1), col = c("red", "black", "green"))




```

**Linear regression - ordinary least squares**

In normal regression we attempt tp reduce the sum of squares residuals. We do this by reducing vertical distance between the observed data points and the regression line. However, this technique assumes that there is no error in the x axis values and heteroscadascity. With my data, both of these assumptions are violated - I have variance in my variances, and I assume my x axis values to be factorial when they are actually a continous variable due to the carry over from the steering wheel reset. 

Because linear regression only reduces vertical residuals, any error in the x axis values is then pushed onto y axis and thus the slope and intercept might not be a true reflection of the data. 

**Deming regression**

Deming regression does not make the assumption that the axis is free of error. Thus residuals are defined as perpendicular to the regression line. In this sense, they compute a regression line for two-dimensional dataset.

**Weighted Deming regression**

If data is heteoscadastic, you can weight the regression. This means that zeroson the axis will be cause the calculation to be compressed. However as I have no zeros on either axis, this will no change regression line.

**Passing Bablok regression**

This form of regression does not minimise residuals per se. Rather, it computes a regression slope every possible pair of points except for pairs that results in a slope of 0 or -1. The final slope values is computed by taking the median of the all point pair slopes. To correct for estimation bias caused by the lack independence of the slopes, the median is shifted by a factor of the number of slopes that result in -1. As a result, this regression is computationally heavy for more than 100 observations, so that it worth bearing in mind.

This method is also more robust to outliers within you data.

**Quantile regression**

The ordinary least squares aims to reduce sum of squared residuals via the vertical distance between the regression line and the obersved data points. This results in estimation of the conditional means of the response variable for a given value of the predicitor.

Conversely, quantile regression aims at estimating the conditional median (or any other quantile you might want). This might be good for my data, as the median is a commonly used central tendency measure for RT data. 

**Iteratively re-weighted least squares regression**

This method down-weights outliers according to how far away from the regression line they are, and iteratively re-fits the model until convergence is achieved. This still does not get away from the fact that error in my x axis gets swallowed up by the y axis.

```{r computing different types of regressions for each subject}

# empty vectors for slope coefficients and intercepts for linear regression
heading_coefs_lin = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)
heading_intercept_lin = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)

# empty vectors for slope coefficients and intercepts for deming regression
heading_coefs_dem = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)
heading_intercept_dem = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)

# empty vectors for slope coefficients and intercepts for passing-bablok regression
heading_coefs_PB = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)
heading_intercept_PB = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)

# empty vectors for slope coefficients and intercepts for quantile regression
heading_coefs_QR = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)
heading_intercept_QR = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)

for (i in c(1:19)){
  # Create temporary data frame:
heading_tmp <- modellingdata[modellingdata$pNum == i,]
  # Perform linear regression:
reg_result <- lm(FirstSteeringTime ~ error_rate, data = heading_tmp)
  # Get coefficient:
tmp_coef <- coef(reg_result)
# Store coefficient and intercept for each subject:
heading_coefs_lin[i] <- tmp_coef[2] 
heading_intercept_lin[i] <- tmp_coef[1]

  # Perform Deming regression:
dem_result <- mcreg(heading_tmp$error_rate, heading_tmp$FirstSteeringTime, method.reg = "Deming")
  # Store coefficient and intercept for each subject:
heading_coefs_dem[i] <- dem.reg@para[2]
heading_intercept_dem[i] <- dem.reg@para[1]

  # Perform Passing-Bablok regression:
PB_result <- mcreg(heading_tmp$error_rate, heading_tmp$FirstSteeringTime, method.reg = "PaBa")
  # Store coefficient and intercept for each subject:
heading_coefs_PB[i] <- PB_result@para[2]
heading_intercept_PB[i] <- PB_result@para[1]

  # Perform Quantile regression:
QR_result <- rq(heading_tmp$FirstSteeringTime ~ heading_tmp$error_rate)
  # Get coffefficient
tmp_coef <- coef(QR_result)
  # Store coefficient and intercept for each subject:
heading_coefs_QR[i] <- tmp_coef[2]
heading_intercept_QR[i] <- tmp_coef[1]


# base R plotting
plot(jitter(heading_tmp$error_rate, 3), heading_tmp$FirstSteeringTime, col = adjustcolor("black", alpha = .3), main = "Regression method comparison")
abline(heading_intercept_lin[i], heading_coefs_lin[i], col = "red")
abline(heading_intercept_dem[i], heading_coefs_dem[i], col = "blue")
abline(heading_intercept_PB[i], heading_coefs_PB[i], col = "green")
abline(heading_intercept_QR[i], heading_coefs_QR[i], col = "black")
legend("topleft", c("Linear", "Deming", "Passing-Bablok", "Quantile"), lty=c(1,1), col = c("red", "blue", "green", "black"))
}

```

